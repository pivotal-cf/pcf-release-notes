---
title: Pivotal Platform v2.8 Feature Highlights
modified_date: false
---

 <%# This list comes from combining, selecting, and summarizing all the release notes, and punching them up a bit, informed by marketing -- see the Wiki https://docs-wiki.cfapps.io/wiki/team/release-manager.html#feature-highlights %>

This topic highlights important new features included in <%= vars.platform_name %> <%= vars.v_major_version %>.


## <a id='om'></a> <%= vars.ops_manager_full %> Highlights

<%= vars.ops_manager %> <%= vars.v_major_version %> includes the important major features. For additional information about these and other features included in <%= vars.ops_manager %> <%= vars.v_major_version %>, see [<%= vars.ops_manager_full %> <%= vars.v_major_version %> Release Notes](opsmanager-rn.html).

### <a id='om-optional-dependencies'></a> Add Optional Dependencies

Tile authors can include both required and optional product dependencies for tiles.

Optional dependencies are only required if you upload both the dependent tile and the optional dependency to your environment.

If tiles use optional dependencies instead of required dependencies, operators do not need to upload tiles in a particular order to avoid errors during deployment.

### <a id='om-time-verifier'></a> HTTP Install-Time Verifier

Tile authors can define an HTTP install-time verifier that calls an HTTP endpoint on the broker. <%= vars.ops_manager %> executes this verifier after you click **Apply Changes**. If the HTTP response is not successful, the deployment fails and the verifier displays a warning message.

Tile authors can use this verifier to check for service instances that might become orphaned after an upgrade.

For more information, see [install\_time\_verifiers](https://docs.pivotal.io/tiledev/2-8/property-template-references.html#top-verifier) in _Property and Template References_ in the <%= vars.platform_name %> Tile Developer Guide.

### <a id='om-system-metrics-agent'></a> System Metrics Agent Installed By Default

<%= vars.ops_manager %> installs a System Metrics Agent on all <%= vars.platform_name %> VMs by default.

System metrics report the usage and status of VM memory, disk, CPU, network, load, and swap space.

Other platform tools such as Pivotal Healthwatch and Pivotal App Metrics can consume VM metrics from <%= vars.app_runtime_abbr %>, <%= vars.k8s_runtime_abbr %>, hosted services, and any other products deployed by <%= vars.ops_manager %>.

For a complete list of metrics, see [VM Metrics](https://github.com/cloudfoundry/system-metrics-release/blob/develop/docs/system-metrics-agent.md#vm-metrics) in _System Metrics Agent_ in the System Metrics repository on GitHub.

### <a id='om-nsx-manager-auth'></a> <%= vars.ops_manager %> Supports Certificate Authentication to vSphere NSX Manager

For <%= vars.platform_name %> deployments on vSphere that use NSX networking, the BOSH Director, NSX-T Container Plugin (NCP), and <%= vars.k8s_runtime_abbr %> can all authenticate to the NSX Manager with a certificate and private key, as well as with a username and password.

For information about how to configure BOSH Director authentication to NSX, see [Step 2: Configure vCenter](https://docs.pivotal.io/platform/ops-manager/2-8/vsphere/config.html#vcenter-config) in _Configuring BOSH Director on vSphere_.

### <a id='om-revert-staged-changes'></a> Revert Staged Changes With the API

You can use the `DELETE /api/v0/staged` <%= vars.ops_manager %> API endpoint to revert all staged changes in <%= vars.ops_manager %>. For more information, see [Revert staged changes](https://docs.pivotal.io/platform/2-8/opsman-api/#streaming-current-installation-log) in the <%= vars.ops_manager %> API documentation.

### <a id='om-multiple-hsm-hosts'></a> Configure Multiple HSMs With the API

You can use the `PUT /api/v0/staged/director/properties` endpoint of the <%= vars.ops_manager %> API to configure multiple hardware security modules (HSMs) for BOSH CredHub. For more information, see [Updating director and Iaas properties (Experimental)](https://docs.pivotal.io/platform/2-8/opsman-api/#updating-director-and-iaas-properties-experimental) in the <%= vars.ops_manager %> API documentation.

### <a id='telemetry'></a> Pivotal Telemetry for <%= vars.ops_manager %> Is Imported by Default

<%= vars.ops_manager %> automatically imports the Pivotal Telemetry for <%= vars.ops_manager %> tile. This tile collects product usage data, which helps Pivotal improve our products and services.

Using Pivotal Telemetry for <%= vars.ops_manager %> is optional, and the tile does not share product usage data until you add and configure it.

For more information, see the [Pivotal Telemetry for <%= vars.ops_manager %>](http://docs.pivotal.io/pivotal-telemetry/index.html) documentation.

---

## <a id='pas'></a> <%= vars.app_runtime_full %> (<%= vars.app_runtime_abbr %>) Highlights

<%= vars.app_runtime_abbr %> <%= vars.v_major_version %> includes the following important major features. For additional information about these and other features included in <%= vars.app_runtime_abbr %> <%= vars.v_major_version %>, see [<%= vars.app_runtime_full %> <%= vars.v_major_version %> Release Notes](runtime-rn.html).

### <a id='deploy-sidecar-buildpack'></a> Deploy Sidecar Processes with a Buildpack

You can deploy a sidecar process for an app with a buildpack rather than with an app manifest.

For more information about deploying sidecar processes with buildpacks, see [Sidecar Buildpacks](https://docs.pivotal.io/platform/application-service/2-8/buildpacks/sidecar-buildpacks.html).

### <a id='cli-support-sidecars'></a> cf CLI Supports Sidecar Processes

The Cloud Foundry Command-Line Interface (cf CLI) adds support for sidecar processes by displaying the sidecar process alongside the app process to which it is attached.

For more information about deploying sidecar processes with apps, see [Pushing Apps with Sidecar Processes (Beta)](https://docs.pivotal.io/platform/application-service/2-8/devguide/sidecars.html).

### <a id='credhub-instance-count'></a> <%= vars.app_runtime_abbr %> Deployed With CredHub by Default

CredHub is now a required component in <%= vars.app_runtime_abbr %>, and
the default number of CredHub instances is increased from `0` to `2`.

You must set the number of CredHub instances to at least `1` when you deploy
<%= vars.app_runtime_abbr %> v2.8. To set the number of CredHub instances
for <%= vars.app_runtime_abbr %>, use the **Resource Config** pane of the
<%= vars.app_runtime_abbr %> tile.

<%= vars.app_runtime_abbr %> components require data stored in CredHub.
For more information about runtime CredHub, see
[Runtime CredHub](https://docs.pivotal.io/platform/2-8/credhub/index.html#runtime)
in _CredHub_.

### <a id='cpu-entitlement'></a> CPU Usage Metric Is Relative to CPU Entitlement for the Container

Garden uses the CPU weight property for a container to calculate a `AbsoluteCPUEntitlement` metric, which is the CPU entitlement for the container.

Garden can then produce CPU usage metrics that are relative to `AbsoluteCPUEntitlement`. For example, a value of `100%` for CPU usage indiciates that the app is using all the CPU to which it is entitled.

CPU usage metrics that are relative to the CPU entitlement for the container help you make more informed scaling decisions for your apps.

For more information about the `AbsoluteCPUEntitlement` metric, see [Diego Container Metrics](http://docs.pivotal.io/platform/application-service/2-8/loggregator/container-metrics.html#container-metrics) in _Container Metrics_.

For information about the Cloud Foundry CPU Entitlement Plugin, an experimental plugin that allows you to examine the CPU usage of <%= vars.app_runtime_abbr %> apps relative to their CPU entitlement, see the [cpu-entitlement-plugin](https://github.com/cloudfoundry/cpu-entitlement-plugin) repository on GitHub.

### <a id='aws-ecr-support'></a> Support for Pushing Container Images Hosted in AWS ECR

When you push container images hosted in AWS Elastic Container Registry (ECR) with the cf CLI, you can provide the access key ID and secret for an AWS IAM user as a Docker username and password.

This update allows the cf CLI to successfully pull container images hosted in ECR with valid AWS Identity and Access Management (IAM) user credentials.

For more information, see [Amazon Elastic Container Registry (ECR)](https://docs.pivotal.io/platform/application-service/2-8/devguide/deploy-apps/push-docker.html#ecr) in _Deploying an App with Docker_.

### <a id='aggregate-syslog-drain'></a> Forward Logs and Metrics for All Apps with Aggregate Syslog Drain

You can configure an aggregate log and metric drain for your foundation to allow Syslog Agents to forward all app metrics, app logs, and VM metrics to one or more syslog endpoints.

This allows you to forward logs and metrics for all apps in your foundation without configuring syslog drains for each app individually.

For more information about enabling an aggregate log and metric drain for your foundation, see [Configure System Logging](https://docs.pivotal.io/platform/application-service/2-8/operating/configure-pas.html#sys-logging) in _Configuring <%= vars.app_runtime_abbr %>_.

### <a id='apps-manager-scs-config'></a> Apps Manager Spring Cloud Services Config Server Integrations

For Spring Cloud Services (SCS) instances, Apps Manager shows the current status of the SCS Config Server on the service instance detail page. You can also use Apps Manager to trigger the Config Server to update app configurations.

This feature provides closer integrations with Spring Cloud, which means that you can work and troubleshoot more quickly.

For more information, see [View and Update Spring Cloud Services Configurations](https://docs.pivotal.io/platform/application-service/2-8/console/manage-apps.html#configure-scs) in _Managing Apps and Service Instances Using Apps Manager_.

### <a id='apps-manager-scs-config'></a> View Quota Information on Apps Manager

You can view quota information for the orgs in the org header in App Manager. This allows you to more quickly find resource usage information for orgs.

### <a id='manageable-upgrades'></a> Use <%= vars.segment_runtime_full %> to Make Upgrades More Manageable

You can use the <%= vars.segment_runtime_full %> tile to deploy a separate group of Diego Cells without isolating the Diego Cell capacity from other apps. You may want to do this if you have a <%= vars.app_runtime_abbr %> tile with a large volume of Diego Cells.

Putting more of your workloads on the Diego Cells of one or more <%= vars.segment_runtime_full %> tiles has the following benefits:

- Separates the upgrade of the <%= vars.app_runtime_abbr %> control plane from the upgrade of the Diego Cells
- Separates the upgrade of the Diego Cells into smaller groups

For more information, see [Use <%= vars.segment_runtime_full %> to Improve Upgrades for Large Foundations](https://docs.pivotal.io/platform/2-8/release-notes/segment-rn.html#compute-networking-isolation) in _<%= vars.segment_runtime_full %> v2.8 Release Notes_.

---

## <a id='pasw'></a> <%= vars.windows_runtime_full %> (<%= vars.windows_runtime_abbr %>) Highlights

<%= vars.windows_runtime_abbr %> <%= vars.v_major_version %> includes the following important major features. For additional information about these and other features included in <%= vars.windows_runtime_abbr %> <%= vars.v_major_version %>, see [<%= vars.windows_runtime_full %> <%= vars.v_major_version %> Release Notes](windows-rn.html).

### <a id='web-config-transform-buildpack'></a> Web Config Transform Extension Buildpack

You can use the Web Config Transform Extension Buildpack to externalize .NET Framework configurations in the `web.config` file to external sources such as GitHub, CredHub, or environment variables. The buildpack uses token replacement to ensure that app configurations are not included in the `web.config` build artifact.

For more information about using the buildpack, see the [Web Config Transform Buildpack](https://github.com/cloudfoundry-community/web-config-transform-buildpack) repository on GitHub.

---

## <a id='pks'></a> <%= vars.k8s_runtime_full %> (<%= vars.k8s_runtime_abbr %>) v1.6 Highlights

<%= vars.k8s_runtime_abbr %> v1.6 includes the following important major features. additional information about these and other features included in <%= vars.k8s_runtime_abbr %> v1.6, see [Release Notes](https://docs.pivotal.io/pks/1-6/release-notes.html) in the <%= vars.k8s_runtime_abbr %> documentation.

### <a id='pks-tanzu-mc'></a> Experimental Integration with Tanzu Mission Control

<%= vars.k8s_runtime_abbr %> v1.6 includes an experimental integration with Tanzu Mission Control.

For more information, see [Tanzu Mission Control Integration](https://docs.pivotal.io/pks/1-6/installing-nsx-t.html#tmc).

### <a id='pks-cluster-limit'></a> Operators Can Limit Cluster Provisioning

Operators can limit the total number of clusters a user can provision in <%= vars.k8s_runtime_abbr %>.

For more information about quotas, see [Managing Resource Usage with Quotas](https://docs.pivotal.io/pks/1-6/resource-usage.html) and [Viewing Usage Quotas](https://docs.pivotal.io/pks/1-6/resource-review.html).

### <a id='pks-management-console'></a> <%= vars.k8s_runtime_abbr %> Management Console

<%= vars.k8s_runtime_abbr %> v1.6 includes the VMware <%= vars.k8s_runtime_abbr %> Management Console v1.1 installer. VMware <%= vars.k8s_runtime_abbr %> Management Console provides a unified installation experience for deploying <%= vars.k8s_runtime_abbr %> to vSphere. For more information, see [Using the <%= vars.k8s_runtime_abbr %> Management Console](https://docs.pivotal.io/pks/1-6/console/console-index.html).

### <a id='pks-admin-role'></a> Read-Only Admin Role

<%= vars.k8s_runtime_abbr %> v1.6 adds a new UAA scope, `pks.clusters.admin.read`, for <%= vars.k8s_runtime_abbr %> users. Accounts with this scope can access any information about all clusters except for cluster credentials.

For information about UAA scopes, see [UAA Scopes for <%= vars.k8s_runtime_abbr %> Users](https://docs.pivotal.io/pks/1-6/uaa-scopes.html) and [Managing <%= vars.k8s_runtime_abbr %> Users with UAA](https://docs.pivotal.io/pks/1-6/manage-users.html).

### <a id='pks-cluster-config'></a> Configure a Cluster With a Docker Registry CA Certificate (Beta)

Operators can configure a single Kubernetes cluster with a specific Docker Registry CA certificate.

For more information about configuring a cluster with a Docker Registry CA certificate, see [Configuring <%= vars.k8s_runtime_abbr %> Clusters with Private Docker Registry CA Certificates (Beta)](https://docs.pivotal.io/pks/1-6/docker-custom-ca-certs.html).

### <a id='pks-cluster-upgrade'></a> Upgrade Multiple Clusters Simultaneously

Operators can save time on cluster upgrades by upgrading multiple Kubernetes clusters simultaneously. Operators can also designate specific upgrade clusters as canary clusters. Cluster upgrades can be serial, serial with some clusters designated as canary clusters, or in parallel.

For more information about multiple cluster upgrades, see [Upgrade Multiple Kubernetes Clusters](https://docs.pivotal.io/pks/1-6/upgrade-clusters.html#upgrade-clusters-multi) in _Upgrading Clusters_.

### <a id='pks-cluster-creation-time'></a> Accelerated Cluster Creation Time

In the **<%= vars.k8s_runtime_abbr %> API** configuration pane, the **Worker VM Max in Flight** default value is increased from `1` to `4`. This accelerates cluster creation by allowing up to four new nodes to be provisioned simultaneously.

The updated default value is only applied during new <%= vars.k8s_runtime_abbr %> installation and is not applied during an <%= vars.k8s_runtime_abbr %> upgrade.

If you are upgrading <%= vars.k8s_runtime_abbr %> from a previous version and want to accelerate multi-cluster provisioning, you can increase the value of **Worker VM Max in Flight** manually.

### <a id='pks-cluster-creation-time'></a> Support for Active-Active T0

You can use active-active mode on the tier 0 router in both automated-NAT deployments and in Bring Your Own Topology deployments with No-NAT configurations.

For more information, see [Configure Networking](https://docs.pivotal.io/pks/1-6/console/deploy-ent-pks-wizard.html#networking) in _Deploy <%= vars.k8s_runtime_abbr %> by Using the Configuration Wizard_.

### <a id='pks-lb-tier-1-routers'></a> Use Different Failure Domains for Load Balancer and Tier-1 Active/Standby Routers

You can place the load balancer and Tier-1 Active/Standby routers on different failure domains.

For more information, see [Multisite Deployment of NSX-T Data Center](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.5/administration/GUID-5D7E3D43-6497-4273-99C1-77613C36AD75.html) in the VMware documentation.

### <a id='pks-health-status'></a> View Health Status of NSX-T Cluster Networking Object

NSX Error CRD allows cluster managers and users to view NSX errors in Kubernetes resource annotations. The command `kubectl get nsxerror` provides the health status of NSX-T cluster networking objects for NCP v2.5.0 and later.

This improves visibility and troubleshooting for cluster managers and users.

For more information, see [Viewing the Health Status of Cluster Networking Objects (NSX-T only)](https://docs.pivotal.io/pks/1-6/nsxt-health.html).

### <a id='pks-ingress-scale'></a> Scale Ingress Capacity With the Load Balancer CRD

The NSXLoadBalancerMonitor CRD allows you monitor the load balancer and ingress resource capacity.

For NCP v2.5.1 and later, you can run the command `kubectl get nsxLoadBalancerMonitors` to view a health score that reflects the current performance of the NSX-T load balancer service, including usage, traffic, and current status.

For more information, see [Ingress Scaling (NSX-T only)](https://docs.pivotal.io/pks/1-6/nsxt-ingress-scale.html).
